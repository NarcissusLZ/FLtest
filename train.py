import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import os
import time
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import datetime

from vgg16 import CIFAR100_VGG16, select_device

# ================= é…ç½®å‚æ•° =================
CONFIG = {
    'device': 'cuda',
    'epochs': 50,
    'batch_size': 64,
    'lr': 0.01,
    'momentum': 0.9,
    'weight_decay': 5e-4,
    'save_path': './checkpoints_cifar100',
    'analysis_path': './analysis_results_cifar100',
    'log_file': 'layer_split_log_fix.txt',
    'num_workers': 2
}


def get_data_loaders(batch_size, num_workers):
    print("æ­£åœ¨å‡†å¤‡ CIFAR-100 æ•°æ®...")
    cifar100_mean = (0.5071, 0.4867, 0.4408)
    cifar100_std = (0.2675, 0.2565, 0.2761)
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(cifar100_mean, cifar100_std),
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(cifar100_mean, cifar100_std),
    ])
    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    return trainloader, testloader


def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch + 1} Train', leave=False)
    for i, (inputs, labels) in pbar:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        pbar.set_postfix({'Loss': f'{running_loss / (i + 1):.4f}', 'Acc': f'{100. * correct / total:.2f}%'})
    return running_loss / len(dataloader), 100. * correct / total


def evaluate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    return running_loss / len(dataloader), 100. * correct / total


def log_and_print(message, log_path):
    print(message)
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(message + '\n')


def record_layer_metrics(model, epoch, history_list):
    for name, param in model.named_parameters():
        if 'weight' in name and len(param.shape) > 1:
            l2_val = param.norm(p=2).item()
            history_list.append({
                'epoch': epoch + 1,
                'layer': name,
                'l2_norm': l2_val
            })


def save_and_plot_analysis(history_list, save_dir):
    if not os.path.exists(save_dir): os.makedirs(save_dir)
    df = pd.DataFrame(history_list)
    df.to_csv(os.path.join(save_dir, 'training_metrics_fix.csv'), index=False)
    # ç®€å•çš„ L2 è¶‹åŠ¿å›¾
    plt.figure(figsize=(12, 8))
    for layer_name in df['layer'].unique():
        layer_data = df[df['layer'] == layer_name]
        short_name = layer_name.replace('features.', 'F').replace('dense.', 'D')
        plt.plot(layer_data['epoch'], layer_data['l2_norm'], label=short_name, marker='o', markersize=3)
    plt.title('Layer L2 Norm Evolution')
    plt.savefig(os.path.join(save_dir, 'l2_fix_plot.png'), dpi=300)


# ================= ã€æ ¸å¿ƒä¿®æ”¹ã€‘ L2 + ç»“æ„æƒé‡ + å¯¹æ•°èšç±» =================

def kmeans_split_log_space(values):
    """
    åœ¨å¯¹æ•°ç©ºé—´è¿›è¡Œ K-Means (k=2)ï¼Œèƒ½æ›´å¥½åœ°å¤„ç†æ•°é‡çº§å·®å¼‚
    æ¯”å¦‚: [6, 12, 20] -> Log: [1.8, 2.5, 3.0]
    Threshold ~ 2.4 (Log) -> ~11 (Linear)
    è¿™æ · 12 å’Œ 20 éƒ½ä¼šè¢«åˆ’åˆ†ä¸º Highï¼Œè€Œ 6 æ˜¯ Lowã€‚
    """
    data = np.array(values)
    # é¿å… log(0)
    data_log = np.log(data + 1e-6).reshape(-1, 1)

    if len(data) < 2: return np.exp(data_log[0][0])

    c1, c2 = np.min(data_log), np.max(data_log)
    for _ in range(10):
        dist1 = np.abs(data_log - c1)
        dist2 = np.abs(data_log - c2)
        group1 = data_log[dist1 <= dist2]
        group2 = data_log[dist1 > dist2]
        new_c1 = group1.mean() if len(group1) > 0 else c1
        new_c2 = group2.mean() if len(group2) > 0 else c2
        if c1 == new_c1 and c2 == new_c2: break
        c1, c2 = new_c1, new_c2

    thresh_log = (c1 + c2) / 2
    return np.exp(thresh_log)  # è¿˜åŸå›çº¿æ€§ç©ºé—´


def classify_layers_fix(model):
    layer_scores = {}
    score_values = []

    for name, param in model.named_parameters():
        if 'weight' in name:
            # 1. è¿‡æ»¤ BN å±‚
            if len(param.shape) <= 1: continue

            # 2. å›å½’å•çº¯çš„ L2 èŒƒæ•° (Dense å±‚ä¼šå¤©ç„¶å¾ˆé«˜)
            l2_val = param.norm(p=2).item()

            # 3. ç»“æ„æ€§åŠ æƒ (Structural Weighting)
            # è¿™é‡Œçš„ç›®çš„æ˜¯è®© First Layer ä¹Ÿèƒ½è¾¾åˆ° Dense Layer çš„æ•°é‡çº§
            weighted_l2 = l2_val

            if "features.0" in name:
                weighted_l2 *= 4.0  # ç¬¬ä¸€å±‚ L2é€šå¸¸~6, x4å~24 (åª²ç¾Dense)
            elif "classifier" in name:
                weighted_l2 *= 2.0  # åˆ†ç±»å¤´
            # æ·±å±‚å·ç§¯ (Deep Conv) å’Œ å…¨è¿æ¥ (Dense) ä¸éœ€è¦åŠ æƒ
            # å› ä¸º Deep Conv L2 é€šå¸¸ ~12ï¼ŒDense L2 é€šå¸¸ ~20
            # å®ƒä»¬è‡ªç„¶æ¯” Shallow Conv (~6) é«˜ï¼Œä¼šè¢«å¯¹æ•°èšç±»è‡ªåŠ¨åˆ†åˆ° High ç»„

            layer_scores[name] = weighted_l2
            score_values.append(weighted_l2)

    # 4. å¯¹æ•°ç©ºé—´åŠ¨æ€åˆ’åˆ†
    threshold = kmeans_split_log_space(score_values)

    critical = []
    robust = []

    for name, val in layer_scores.items():
        if val >= threshold:
            critical.append(f"{name}")
        else:
            robust.append(f"{name}")

    return critical, robust, threshold


# ================= ä¸»å‡½æ•° =================
def main():
    device = select_device(CONFIG['device'])
    if not os.path.exists(CONFIG['save_path']): os.makedirs(CONFIG['save_path'])
    if not os.path.exists(CONFIG['analysis_path']): os.makedirs(CONFIG['analysis_path'])

    log_path = os.path.join(CONFIG['analysis_path'], CONFIG['log_file'])
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write(f"Training Log (Fix) - {datetime.datetime.now()}\n")
        f.write("Strategy: L2 Norm (Base) + Log-Space Clustering + First Layer Boost\n")
        f.write("=" * 60 + "\n")

    trainloader, testloader = get_data_loaders(CONFIG['batch_size'], CONFIG['num_workers'])
    model = CIFAR100_VGG16(num_classes=100).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=CONFIG['lr'], momentum=CONFIG['momentum'],
                          weight_decay=CONFIG['weight_decay'])
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)
    history = []

    log_and_print(f"å¼€å§‹è®­ç»ƒ (Fix ç­–ç•¥)...", log_path)
    start_time = time.time()

    for epoch in range(CONFIG['epochs']):
        train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer, device, epoch)
        val_loss, val_acc = evaluate(model, testloader, criterion, device)

        record_layer_metrics(model, epoch, history)
        critical_layers, robust_layers, thresh = classify_layers_fix(model)

        msg = []
        msg.append(f"\n[{epoch + 1}/{CONFIG['epochs']}] Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%")
        msg.append(f"Weighted L2 Threshold: {thresh:.2f}")
        msg.append(f"ğŸ”´ Critical (TCP): {', '.join(critical_layers)}")
        msg.append(f"ğŸŸ¢ Robust (UDP):  {', '.join(robust_layers)}")
        msg.append("-" * 60)

        log_and_print("\n".join(msg), log_path)
        scheduler.step()

    total_time = time.time() - start_time
    log_and_print(f"\nè®­ç»ƒç»“æŸï¼Œè€—æ—¶ {total_time / 60:.2f} åˆ†é’Ÿã€‚", log_path)
    save_and_plot_analysis(history, CONFIG['analysis_path'])


if __name__ == '__main__':
    main()