import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import os
import time
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import datetime

# å¯¼å…¥ä½ çš„æ¨¡å‹æ–‡ä»¶
# ç¡®ä¿ vgg16.py åœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œæˆ–è€…è°ƒæ•´å¼•ç”¨è·¯å¾„
from vgg16 import CIFAR10_VGG16, select_device

# ================= é…ç½®å‚æ•° =================
CONFIG = {
    'device': 'cuda',
    'epochs': 50,
    'batch_size': 64,
    'lr': 0.01,
    'momentum': 0.9,
    'weight_decay': 5e-4,
    'save_path': './checkpoints',
    'analysis_path': './analysis_results',
    'log_file': 'layer_split_ratio_log.txt',  # æ—¥å¿—æ–‡ä»¶åå·²æ›´æ–°
    'num_workers': 2,
    'critical_ratio': 0.3  # ã€æ–°å¢ã€‘å…³é”®å±‚æ¯”ä¾‹ï¼šå‰ 30% (L2æœ€å¤§çš„) èµ° TCP
}


def get_data_loaders(batch_size, num_workers):
    """å‡†å¤‡ CIFAR-10 æ•°æ®é›†"""
    print("æ­£åœ¨å‡†å¤‡æ•°æ®...")
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    return trainloader, testloader


def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch + 1} Train', leave=False)
    for i, (inputs, labels) in pbar:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        pbar.set_postfix({'Loss': f'{running_loss / (i + 1):.4f}', 'Acc': f'{100. * correct / total:.2f}%'})
    return running_loss / len(dataloader), 100. * correct / total


def evaluate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    return running_loss / len(dataloader), 100. * correct / total


# ================= è¾…åŠ©åŠŸèƒ½ï¼šåŒé‡æ—¥å¿—è®°å½• =================
def log_and_print(message, log_path):
    """
    æ—¢æ‰“å°åˆ°æ§åˆ¶å°ï¼Œä¹Ÿè¿½åŠ å†™å…¥æ–‡ä»¶
    """
    print(message)  # æ§åˆ¶å°è¾“å‡º
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(message + '\n')  # æ–‡ä»¶å†™å…¥


# ================= æ ¸å¿ƒé€»è¾‘ï¼šTop-K æ¯”ä¾‹åˆ†å±‚ç®—æ³• (å¿½ç•¥BN) =================
def classify_layers_by_ratio(model, ratio):
    """
    æŒ‰L2èŒƒæ•°ä»å¤§åˆ°å°æ’åºï¼Œå–å‰ ratio% ä¸ºå…³é”®å±‚ã€‚
    å¿½ç•¥ BN å±‚ (é€šè¿‡ç»´åº¦åˆ¤æ–­ï¼ŒBN weightæ˜¯1ç»´ï¼ŒConv/Linearæ˜¯2ç»´ä»¥ä¸Š)
    """
    layer_info = []

    for name, param in model.named_parameters():
        # 1. ç­›é€‰é€»è¾‘ï¼š
        # 'weight' in name: æ’é™¤ bias
        # param.dim() > 1: æ’é™¤ BN çš„ weight (BNçš„weightæ˜¯1ç»´çš„ï¼ŒConv/Linearæ˜¯2ç»´æˆ–4ç»´)
        if 'weight' in name and param.dim() > 1:
            val = param.norm(p=2).item()
            layer_info.append((name, val))

    # 2. æ’åºï¼šä»å¤§åˆ°å° (High to Low)
    layer_info.sort(key=lambda x: x[1], reverse=True)

    # 3. è®¡ç®—åˆ‡åˆ†ç‚¹
    num_total = len(layer_info)
    num_critical = int(num_total * ratio)

    # è¾¹ç•Œä¿æŠ¤ï¼šå¦‚æœæ¯”ä¾‹>0ä½†è®¡ç®—ç»“æœä¸º0ï¼Œè‡³å°‘ä¿ç•™1å±‚
    if num_critical == 0 and ratio > 0 and num_total > 0:
        num_critical = 1

    # 4. åˆ‡åˆ†åˆ—è¡¨
    critical_list = layer_info[:num_critical]
    robust_list = layer_info[num_critical:]

    # 5. æ ¼å¼åŒ–è¾“å‡º (Name, Value)
    critical_desc = [f"{n} ({v:.2f})" for n, v in critical_list]
    robust_desc = [f"{n} ({v:.2f})" for n, v in robust_list]

    # è·å–å½“å‰çš„åˆ†ç•Œé˜ˆå€¼ï¼ˆå³å…³é”®å±‚ä¸­æœ€å°çš„é‚£ä¸ªå€¼ï¼Œç”¨äºæ—¥å¿—å±•ç¤ºï¼‰
    # å¦‚æœ critical_list ä¸ºç©ºï¼Œåˆ™é˜ˆå€¼ä¸ºæ— ç©·å¤§æˆ–0
    current_threshold = critical_list[-1][1] if critical_list else 0.0

    return critical_desc, robust_desc, current_threshold


def record_layer_l2_norms(model, epoch, history_list):
    """è®°å½•æ•°æ®ç”¨äºäº‹åç»˜å›¾"""
    for name, param in model.named_parameters():
        # åŒæ ·åªè®°å½• Conv/Linear çš„æƒé‡ï¼Œä¿æŒé€»è¾‘ä¸€è‡´
        if 'weight' in name and param.dim() > 1:
            l2_val = param.norm(p=2).item()
            history_list.append({
                'epoch': epoch + 1,
                'layer': name,
                'l2_norm': l2_val
            })


def save_and_plot_analysis(history_list, save_dir):
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    df = pd.DataFrame(history_list)
    csv_path = os.path.join(save_dir, 'training_l2_history.csv')
    df.to_csv(csv_path, index=False)

    plt.figure(figsize=(12, 8))
    layers = df['layer'].unique()
    for layer_name in layers:
        layer_data = df[df['layer'] == layer_name]
        short_name = layer_name.replace('features.', 'F').replace('dense.', 'D').replace('classifier.', 'C')
        plt.plot(layer_data['epoch'], layer_data['l2_norm'], label=short_name, marker='o', markersize=3)
    plt.title('Layer L2 Norm Evolution During Training')
    plt.xlabel('Epoch')
    plt.ylabel('L2 Norm')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'l2_evolution_plot.png'), dpi=300)


# ================= ä¸»å‡½æ•° =================
def main():
    device = select_device(CONFIG['device'])
    print(f"Using device: {device}")

    if not os.path.exists(CONFIG['save_path']): os.makedirs(CONFIG['save_path'])
    if not os.path.exists(CONFIG['analysis_path']): os.makedirs(CONFIG['analysis_path'])

    # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_path = os.path.join(CONFIG['analysis_path'], CONFIG['log_file'])

    # å†™å…¥æ—¥å¿—å¤´
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write(f"Training Log - Started at {datetime.datetime.now()}\n")
        f.write(f"Strategy: Dynamic Split based on Top {CONFIG['critical_ratio'] * 100}% Ratio (Ignoring BN)\n")
        f.write("=" * 60 + "\n")

    trainloader, testloader = get_data_loaders(CONFIG['batch_size'], CONFIG['num_workers'])
    model = CIFAR10_VGG16(num_classes=10).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=CONFIG['lr'], momentum=CONFIG['momentum'],
                          weight_decay=CONFIG['weight_decay'])
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)

    l2_history = []

    log_and_print(f"å¼€å§‹è®­ç»ƒ {CONFIG['epochs']} è½®...", log_path)
    log_and_print(f"æ—¥å¿—æ–‡ä»¶ä½ç½®: {log_path}", log_path)

    start_time = time.time()

    for epoch in range(CONFIG['epochs']):
        # 1. è®­ç»ƒ
        train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer, device, epoch)

        # 2. éªŒè¯
        val_loss, val_acc = evaluate(model, testloader, criterion, device)

        # 3. è®°å½•å†å²æ•°æ®
        record_layer_l2_norms(model, epoch, l2_history)

        # 4. ã€å®æ—¶è¾“å‡ºã€‘ è®¡ç®—å¹¶æ‰“å°æœ¬è½®çš„åˆ†å±‚ç»“æœ (ä½¿ç”¨æ–°çš„æ¯”ä¾‹å‡½æ•°)
        critical_layers, robust_layers, thresh = classify_layers_by_ratio(model, CONFIG['critical_ratio'])

        # æ„å»ºè¦æ‰“å°å’Œä¿å­˜çš„æ—¥å¿—ä¿¡æ¯
        msg = []
        msg.append(f"\n[{epoch + 1}/{CONFIG['epochs']}] Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%")
        msg.append(f"=" * 20 + f" åŠ¨æ€åˆ†å±‚ (Top {CONFIG['critical_ratio'] * 100:.0f}%) " + "=" * 20)
        msg.append(f"å½“å‰åˆ†ç•Œé˜ˆå€¼ (Min Critical L2): {thresh:.4f}")

        msg.append(f"ğŸ”´ å…³é”®å±‚ (Critical/TCP, Count={len(critical_layers)}):")
        # è®°å½•æ‰€æœ‰å…³é”®å±‚åå­—ï¼Œæ ¼å¼åŒ–å»æ‰å¤šä½™çš„ç©ºæ ¼
        msg.append(", ".join([x.split(' ')[0] for x in critical_layers]))

        msg.append(f"ğŸŸ¢ é²æ£’å±‚ (Robust/UDP, Count={len(robust_layers)}):")
        # é²æ£’å±‚
        msg.append(", ".join([x.split(' ')[0] for x in robust_layers]))

        msg.append("=" * 60)

        # å°†ä¸Šé¢æ„å»ºçš„æ‰€æœ‰ä¿¡æ¯ä¸€æ¬¡æ€§è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
        log_and_print("\n".join(msg), log_path)

        scheduler.step()

    total_time = time.time() - start_time
    final_msg = f"\nè®­ç»ƒç»“æŸï¼Œè€—æ—¶ {total_time / 60:.2f} åˆ†é’Ÿã€‚"
    log_and_print(final_msg, log_path)

    save_and_plot_analysis(l2_history, CONFIG['analysis_path'])


if __name__ == '__main__':
    main()