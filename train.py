import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import os
import time
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import datetime

# å¯¼å…¥ä½ çš„æ¨¡å‹æ–‡ä»¶
from vgg16 import CIFAR10_VGG16, select_device

# ================= é…ç½®å‚æ•° =================
CONFIG = {
    'device': 'cuda',  # 'cuda', 'mps' (Mac), or 'cpu'
    'epochs': 50,  # è®­ç»ƒè½®æ•°
    'batch_size': 64,  # æ‰¹æ¬¡å¤§å°
    'lr': 0.01,  # åˆå§‹å­¦ä¹ ç‡
    'momentum': 0.9,  # SGD åŠ¨é‡
    'weight_decay': 5e-4,  # æƒé‡è¡°å‡ (L2æ­£åˆ™åŒ–)
    'save_path': './checkpoints',  # æ¨¡å‹ä¿å­˜è·¯å¾„
    'analysis_path': './analysis_results',  # åˆ†æç»“æœä¿å­˜è·¯å¾„
    'log_file': 'layer_split_log.txt',  # æ—¥å¿—æ–‡ä»¶
    'num_workers': 2  # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
}


# ================= 1. æ•°æ®å‡†å¤‡ =================
def get_data_loaders(batch_size, num_workers):
    print("æ­£åœ¨å‡†å¤‡ CIFAR-10 æ•°æ®...")

    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

    return trainloader, testloader


# ================= 2. è®­ç»ƒä¸è¯„ä¼°å‡½æ•° =================
def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch + 1} Train', leave=False)

    for i, (inputs, labels) in pbar:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        pbar.set_postfix({'Loss': f'{running_loss / (i + 1):.4f}', 'Acc': f'{100. * correct / total:.2f}%'})

    return running_loss / len(dataloader), 100. * correct / total


def evaluate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in tqdm(dataloader, desc='Evaluating', leave=False):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    return running_loss / len(dataloader), 100. * correct / total


# ================= 3. æ ¸å¿ƒåŠŸèƒ½ï¼šL2èŒƒæ•°åˆ†å±‚é€»è¾‘ =================

def log_and_print(message, log_path):
    """åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    print(message)
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(message + '\n')


def kmeans_split_log_space(values):
    """
    å¯¹æ•°ç©ºé—´ K-Means èšç±»ã€‚
    è§£å†³ VGG ä¸­å…¨è¿æ¥å±‚ L2 (20+) ä¸å·ç§¯å±‚ L2 (5~12) æ•°é‡çº§å·®å¼‚å¤§çš„é—®é¢˜ã€‚
    """
    data = np.array(values)
    # è½¬åˆ°å¯¹æ•°ç©ºé—´: log(x)
    data_log = np.log(data + 1e-6).reshape(-1, 1)

    if len(data) < 2: return np.exp(data_log[0][0])

    # K-Means (k=2)
    c1, c2 = np.min(data_log), np.max(data_log)
    for _ in range(10):
        dist1 = np.abs(data_log - c1)
        dist2 = np.abs(data_log - c2)
        group1 = data_log[dist1 <= dist2]
        group2 = data_log[dist1 > dist2]
        new_c1 = group1.mean() if len(group1) > 0 else c1
        new_c2 = group2.mean() if len(group2) > 0 else c2
        if c1 == new_c1 and c2 == new_c2: break
        c1, c2 = new_c1, new_c2

    thresh_log = (c1 + c2) / 2
    return np.exp(thresh_log)  # è¿˜åŸå›çº¿æ€§ç©ºé—´


def classify_layers(model):
    """
    è®¡ç®—æ¯ä¸€å±‚çš„ L2 èŒƒæ•°ï¼Œå¹¶æ ¹æ®åˆ†å¸ƒåŠ¨æ€åˆ’åˆ†ä¸º Critical å’Œ Robustã€‚
    åŒ…å«ç‰¹å®šå±‚çš„ç»“æ„æ€§åŠ æƒï¼ˆç¬¬ä¸€å±‚å’Œåˆ†ç±»å¤´ï¼‰ã€‚
    """
    layer_scores = {}
    score_values = []

    for name, param in model.named_parameters():
        # åªåˆ†ææƒé‡ï¼Œä¸”å¿½ç•¥ BatchNorm å±‚ (shape ä¸º 1 çš„æ˜¯ä¸€ç»´å‚æ•°)
        if 'weight' in name and len(param.shape) > 1:

            # 1. åŸºç¡€æŒ‡æ ‡ï¼šL2 èŒƒæ•°
            l2_val = param.norm(p=2).item()

            # 2. ç»“æ„æ€§åŠ æƒ (ä¿®æ­£çº¯ç»Ÿè®¡çš„åå·®)
            weighted_l2 = l2_val

            if "features.0" in name:
                # ç¬¬ä¸€å±‚å·ç§¯æå…¶é‡è¦ï¼Œä½†å‚æ•°å°‘L2å°ï¼Œç»™äºˆé«˜æƒé‡
                weighted_l2 *= 4.0
            elif "classifier" in name or "dense.6" in name:
                # è¾“å‡ºå±‚ç›´æ¥å†³å®šç»“æœï¼Œç»™äºˆåŠ æƒ
                weighted_l2 *= 2.0

            layer_scores[name] = weighted_l2
            score_values.append(weighted_l2)

    # 3. è®¡ç®—åŠ¨æ€é˜ˆå€¼
    threshold = kmeans_split_log_space(score_values)

    critical = []
    robust = []

    for name, val in layer_scores.items():
        if val >= threshold:
            critical.append(name)
        else:
            robust.append(name)

    return critical, robust, threshold


def record_metrics(model, epoch, history_list):
    """è®°å½•åŸå§‹æ•°æ®ç”¨äºç»˜å›¾"""
    for name, param in model.named_parameters():
        if 'weight' in name and len(param.shape) > 1:
            l2_val = param.norm(p=2).item()
            history_list.append({
                'epoch': epoch + 1,
                'layer': name,
                'l2_norm': l2_val
            })


def save_analysis(history_list, save_dir):
    if not os.path.exists(save_dir): os.makedirs(save_dir)
    df = pd.DataFrame(history_list)
    df.to_csv(os.path.join(save_dir, 'l2_history.csv'), index=False)
    print(f"L2èŒƒæ•°å†å²æ•°æ®å·²ä¿å­˜è‡³: {save_dir}")


# ================= 4. ä¸»æµç¨‹ =================
def main():
    device = select_device(CONFIG['device'])
    print(f"Using device: {device}")

    if not os.path.exists(CONFIG['save_path']): os.makedirs(CONFIG['save_path'])
    if not os.path.exists(CONFIG['analysis_path']): os.makedirs(CONFIG['analysis_path'])

    # åˆå§‹åŒ–æ—¥å¿—
    log_path = os.path.join(CONFIG['analysis_path'], CONFIG['log_file'])
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write(f"Training Log - {datetime.datetime.now()}\n")
        f.write("Dataset: CIFAR-10\n")
        f.write("Strategy: L2 Norm with Log-Space Clustering & Structural Weights\n")
        f.write("=" * 60 + "\n")

    trainloader, testloader = get_data_loaders(CONFIG['batch_size'], CONFIG['num_workers'])
    model = CIFAR10_VGG16(num_classes=10).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=CONFIG['lr'], momentum=CONFIG['momentum'],
                          weight_decay=CONFIG['weight_decay'])
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)

    history = []
    best_acc = 0.0

    print(f"å¼€å§‹è®­ç»ƒï¼Œæ—¥å¿—å°†å†™å…¥ {log_path} ...")
    start_time = time.time()

    for epoch in range(CONFIG['epochs']):
        # è®­ç»ƒä¸éªŒè¯
        train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer, device, epoch)
        val_loss, val_acc = evaluate(model, testloader, criterion, device)

        # è®°å½•æ•°æ®
        record_metrics(model, epoch, history)

        # ã€æ ¸å¿ƒè°ƒç”¨ã€‘è®¡ç®—åˆ†å±‚
        critical_layers, robust_layers, thresh = classify_layers(model)

        # æ„å»ºæ—¥å¿—ä¿¡æ¯
        msg = []
        msg.append(f"\n[Epoch {epoch + 1}/{CONFIG['epochs']}] Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%")
        msg.append(f"åˆ†å±‚é˜ˆå€¼ (Weighted L2): {thresh:.2f}")
        msg.append(f"ğŸ”´ å…³é”®å±‚ (TCP): {', '.join(critical_layers)}")
        msg.append(f"ğŸŸ¢ é²æ£’å±‚ (UDP): {', '.join(robust_layers)}")
        msg.append("-" * 60)

        # è¾“å‡ºä¸ä¿å­˜
        log_and_print("\n".join(msg), log_path)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), os.path.join(CONFIG['save_path'], 'best_model.pth'))

        scheduler.step()

    total_time = time.time() - start_time
    log_and_print(f"\nè®­ç»ƒç»“æŸï¼Œæ€»è€—æ—¶ {total_time / 60:.2f} åˆ†é’Ÿã€‚æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%", log_path)
    save_analysis(history, CONFIG['analysis_path'])


if __name__ == '__main__':
    main()