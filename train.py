import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import os
import time
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import datetime

# å¯¼å…¥ä½ çš„æ¨¡å‹æ–‡ä»¶
from vgg16 import CIFAR10_VGG16, select_device

# ================= é…ç½®å‚æ•° =================
CONFIG = {
    'device': 'cuda',
    'epochs': 50,
    'batch_size': 64,
    'lr': 0.01,
    'momentum': 0.9,
    'weight_decay': 5e-4,
    'save_path': './checkpoints',
    'analysis_path': './analysis_results',
    'log_file': 'layer_split_log.txt',  # æ–°å¢ï¼šæ—¥å¿—æ–‡ä»¶å
    'num_workers': 2
}


def get_data_loaders(batch_size, num_workers):
    """å‡†å¤‡ CIFAR-10 æ•°æ®é›†"""
    print("æ­£åœ¨å‡†å¤‡æ•°æ®...")
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    return trainloader, testloader


def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch + 1} Train', leave=False)
    for i, (inputs, labels) in pbar:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        pbar.set_postfix({'Loss': f'{running_loss / (i + 1):.4f}', 'Acc': f'{100. * correct / total:.2f}%'})
    return running_loss / len(dataloader), 100. * correct / total


def evaluate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    return running_loss / len(dataloader), 100. * correct / total


# ================= è¾…åŠ©åŠŸèƒ½ï¼šåŒé‡æ—¥å¿—è®°å½• =================
def log_and_print(message, log_path):
    """
    æ—¢æ‰“å°åˆ°æ§åˆ¶å°ï¼Œä¹Ÿè¿½åŠ å†™å…¥æ–‡ä»¶
    """
    print(message)  # æ§åˆ¶å°è¾“å‡º
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(message + '\n')  # æ–‡ä»¶å†™å…¥


# ================= æ ¸å¿ƒé€»è¾‘ï¼šåŠ¨æ€åˆ†å±‚ç®—æ³• =================
def simple_kmeans_split(values):
    """ç®€å•çš„1D K-Means (k=2) å®ç°"""
    data = np.array(values).reshape(-1, 1)
    c1 = np.min(data)
    c2 = np.max(data)
    for _ in range(10):
        dist1 = np.abs(data - c1)
        dist2 = np.abs(data - c2)
        group1 = data[dist1 <= dist2]
        group2 = data[dist1 > dist2]
        new_c1 = group1.mean() if len(group1) > 0 else c1
        new_c2 = group2.mean() if len(group2) > 0 else c2
        if c1 == new_c1 and c2 == new_c2: break
        c1, c2 = new_c1, new_c2
    threshold = (c1 + c2) / 2
    return threshold


def classify_layers_realtime(model):
    """è·å–å½“å‰L2å¹¶åˆ†ç±»"""
    layer_l2 = {}
    l2_values = []

    for name, param in model.named_parameters():
        if 'weight' in name:
            val = param.norm(p=2).item()
            layer_l2[name] = val
            l2_values.append(val)

    threshold = simple_kmeans_split(l2_values)

    critical = []
    robust = []

    for name, val in layer_l2.items():
        if val >= threshold:
            critical.append(f"{name} ({val:.2f})")
        else:
            robust.append(f"{name} ({val:.2f})")

    return critical, robust, threshold


def record_layer_l2_norms(model, epoch, history_list):
    """è®°å½•æ•°æ®ç”¨äºäº‹åç»˜å›¾"""
    for name, param in model.named_parameters():
        if 'weight' in name:
            l2_val = param.norm(p=2).item()
            history_list.append({
                'epoch': epoch + 1,
                'layer': name,
                'l2_norm': l2_val
            })


def save_and_plot_analysis(history_list, save_dir):
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    df = pd.DataFrame(history_list)
    csv_path = os.path.join(save_dir, 'training_l2_history.csv')
    df.to_csv(csv_path, index=False)

    plt.figure(figsize=(12, 8))
    layers = df['layer'].unique()
    for layer_name in layers:
        layer_data = df[df['layer'] == layer_name]
        short_name = layer_name.replace('features.', 'F').replace('dense.', 'D').replace('classifier.', 'C')
        plt.plot(layer_data['epoch'], layer_data['l2_norm'], label=short_name, marker='o', markersize=3)
    plt.title('Layer L2 Norm Evolution During Training')
    plt.xlabel('Epoch')
    plt.ylabel('L2 Norm')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'l2_evolution_plot.png'), dpi=300)


# ================= ä¸»å‡½æ•° =================
def main():
    device = select_device(CONFIG['device'])
    print(f"Using device: {device}")

    if not os.path.exists(CONFIG['save_path']): os.makedirs(CONFIG['save_path'])
    if not os.path.exists(CONFIG['analysis_path']): os.makedirs(CONFIG['analysis_path'])

    # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_path = os.path.join(CONFIG['analysis_path'], CONFIG['log_file'])
    # æ¸…ç©ºä¹‹å‰çš„æ—¥å¿—ï¼ˆå¦‚æœéœ€è¦ä¿ç•™è¿½åŠ ï¼Œå»æ‰è¿™è¡Œ 'w' æ¨¡å¼çš„å†™å…¥ï¼‰
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write(f"Training Log - Started at {datetime.datetime.now()}\n")
        f.write("Strategy: Real-time Dynamic Split based on Pure L2 Norm\n")
        f.write("=" * 60 + "\n")

    trainloader, testloader = get_data_loaders(CONFIG['batch_size'], CONFIG['num_workers'])
    model = CIFAR10_VGG16(num_classes=10).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=CONFIG['lr'], momentum=CONFIG['momentum'],
                          weight_decay=CONFIG['weight_decay'])
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)

    l2_history = []

    log_and_print(f"å¼€å§‹è®­ç»ƒ {CONFIG['epochs']} è½®...", log_path)
    log_and_print(f"æ—¥å¿—æ–‡ä»¶ä½ç½®: {log_path}", log_path)

    start_time = time.time()

    for epoch in range(CONFIG['epochs']):
        # 1. è®­ç»ƒ
        train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer, device, epoch)

        # 2. éªŒè¯
        val_loss, val_acc = evaluate(model, testloader, criterion, device)

        # 3. è®°å½•å†å²æ•°æ®
        record_layer_l2_norms(model, epoch, l2_history)

        # 4. ã€å®æ—¶è¾“å‡ºã€‘ è®¡ç®—å¹¶æ‰“å°æœ¬è½®çš„åˆ†å±‚ç»“æœ
        critical_layers, robust_layers, thresh = classify_layers_realtime(model)

        # æ„å»ºè¦æ‰“å°å’Œä¿å­˜çš„æ—¥å¿—ä¿¡æ¯
        msg = []
        msg.append(f"\n[{epoch + 1}/{CONFIG['epochs']}] Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%")
        msg.append(f"=" * 20 + " åŠ¨æ€åˆ†å±‚ (Only L2) " + "=" * 20)
        msg.append(f"å½“å‰è½®æ¬¡ L2 é˜ˆå€¼ (Threshold): {thresh:.4f}")

        msg.append(f"ğŸ”´ å…³é”®å±‚ (Critical/TCP, Count={len(critical_layers)}):")
        # è®°å½•æ‰€æœ‰å…³é”®å±‚åå­—
        msg.append(", ".join([x.split(' ')[0] for x in critical_layers]))

        msg.append(f"ğŸŸ¢ é²æ£’å±‚ (Robust/UDP, Count={len(robust_layers)}):")
        # é²æ£’å±‚é€šå¸¸è¾ƒå¤šï¼Œå¦‚æœä¸å¸Œæœ›æ—¥å¿—å¤ªé•¿ï¼Œå¯ä»¥åªè®°åå­—
        msg.append(", ".join([x.split(' ')[0] for x in robust_layers]))

        msg.append("=" * 60)

        # å°†ä¸Šé¢æ„å»ºçš„æ‰€æœ‰ä¿¡æ¯ä¸€æ¬¡æ€§è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
        log_and_print("\n".join(msg), log_path)

        scheduler.step()

    total_time = time.time() - start_time
    final_msg = f"\nè®­ç»ƒç»“æŸï¼Œè€—æ—¶ {total_time / 60:.2f} åˆ†é’Ÿã€‚"
    log_and_print(final_msg, log_path)

    save_and_plot_analysis(l2_history, CONFIG['analysis_path'])


if __name__ == '__main__':
    main()