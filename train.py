import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import os
import time
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np

# å¯¼å…¥ä½ çš„æ¨¡å‹æ–‡ä»¶
from vgg16 import CIFAR10_VGG16, select_device

# ================= é…ç½®å‚æ•° =================
CONFIG = {
    'device': 'cuda',
    'epochs': 50,
    'batch_size': 64,
    'lr': 0.01,
    'momentum': 0.9,
    'weight_decay': 5e-4,
    'save_path': './checkpoints',
    'analysis_path': './analysis_results',
    'num_workers': 2
}


def get_data_loaders(batch_size, num_workers):
    """å‡†å¤‡ CIFAR-10 æ•°æ®é›†"""
    print("æ­£åœ¨å‡†å¤‡æ•°æ®...")
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    return trainloader, testloader


def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch + 1} Train', leave=False)
    for i, (inputs, labels) in pbar:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        pbar.set_postfix({'Loss': f'{running_loss / (i + 1):.4f}', 'Acc': f'{100. * correct / total:.2f}%'})
    return running_loss / len(dataloader), 100. * correct / total


def evaluate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    return running_loss / len(dataloader), 100. * correct / total


# ================= æ ¸å¿ƒé€»è¾‘ï¼šåŠ¨æ€åˆ†å±‚ç®—æ³• =================

def simple_kmeans_split(values):
    """
    ç®€å•çš„1D K-Means (k=2) å®ç°ï¼Œç”¨äºå°†L2èŒƒæ•°åˆ†ä¸ºé«˜ä½ä¸¤ç»„ã€‚
    æ— éœ€ä¾èµ–sklearnï¼Œçº¯numpyå®ç°ã€‚
    """
    data = np.array(values).reshape(-1, 1)

    # åˆå§‹åŒ–ä¸­å¿ƒï¼šæœ€å°å€¼å’Œæœ€å¤§å€¼
    c1 = np.min(data)
    c2 = np.max(data)

    for _ in range(10):  # è¿­ä»£10æ¬¡é€šå¸¸è¶³å¤Ÿæ”¶æ•›
        # è®¡ç®—è·ç¦»
        dist1 = np.abs(data - c1)
        dist2 = np.abs(data - c2)

        # åˆ†é…ç°‡
        group1 = data[dist1 <= dist2]
        group2 = data[dist1 > dist2]

        # æ›´æ–°ä¸­å¿ƒ
        new_c1 = group1.mean() if len(group1) > 0 else c1
        new_c2 = group2.mean() if len(group2) > 0 else c2

        if c1 == new_c1 and c2 == new_c2:
            break
        c1, c2 = new_c1, new_c2

    # ç¡®å®šé˜ˆå€¼ï¼šä¸¤ä¸ªä¸­å¿ƒçš„ä¸­é—´ç‚¹
    threshold = (c1 + c2) / 2
    return threshold


def classify_layers_realtime(model):
    """
    è·å–å½“å‰æ‰€æœ‰å±‚L2èŒƒæ•°ï¼Œå¹¶è¿›è¡Œå®æ—¶åˆ†ç±»
    """
    layer_l2 = {}
    l2_values = []

    # 1. æ”¶é›†æ•°æ®
    for name, param in model.named_parameters():
        if 'weight' in name:
            val = param.norm(p=2).item()
            layer_l2[name] = val
            l2_values.append(val)

    # 2. è®¡ç®—åŠ¨æ€é˜ˆå€¼ (åªåŸºäºL2)
    threshold = simple_kmeans_split(l2_values)

    # 3. åˆ†ç±»
    critical = []
    robust = []

    for name, val in layer_l2.items():
        if val >= threshold:
            critical.append(f"{name} ({val:.2f})")
        else:
            robust.append(f"{name} ({val:.2f})")

    return critical, robust, threshold


def record_layer_l2_norms(model, epoch, history_list):
    """è®°å½•æ•°æ®ç”¨äºäº‹åç»˜å›¾"""
    for name, param in model.named_parameters():
        if 'weight' in name:
            l2_val = param.norm(p=2).item()
            history_list.append({
                'epoch': epoch + 1,
                'layer': name,
                'l2_norm': l2_val
            })


def save_and_plot_analysis(history_list, save_dir):
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    df = pd.DataFrame(history_list)
    csv_path = os.path.join(save_dir, 'training_l2_history.csv')
    df.to_csv(csv_path, index=False)

    plt.figure(figsize=(12, 8))
    layers = df['layer'].unique()
    for layer_name in layers:
        layer_data = df[df['layer'] == layer_name]
        short_name = layer_name.replace('features.', 'F').replace('dense.', 'D').replace('classifier.', 'C')
        plt.plot(layer_data['epoch'], layer_data['l2_norm'], label=short_name, marker='o', markersize=3)
    plt.title('Layer L2 Norm Evolution During Training')
    plt.xlabel('Epoch')
    plt.ylabel('L2 Norm')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'l2_evolution_plot.png'), dpi=300)


# ================= ä¸»å‡½æ•° =================
def main():
    device = select_device(CONFIG['device'])
    print(f"Using device: {device}")

    if not os.path.exists(CONFIG['save_path']): os.makedirs(CONFIG['save_path'])
    if not os.path.exists(CONFIG['analysis_path']): os.makedirs(CONFIG['analysis_path'])

    trainloader, testloader = get_data_loaders(CONFIG['batch_size'], CONFIG['num_workers'])
    model = CIFAR10_VGG16(num_classes=10).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=CONFIG['lr'], momentum=CONFIG['momentum'],
                          weight_decay=CONFIG['weight_decay'])
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)

    l2_history = []

    print(f"å¼€å§‹è®­ç»ƒ {CONFIG['epochs']} è½®...")
    print("æ³¨æ„ï¼šæ¯è½®ç»“æŸåå°†åŸºäºçº¯ L2 èŒƒæ•°åŠ¨æ€è¾“å‡ºå…³é”®å±‚/é²æ£’å±‚åˆ’åˆ†")

    start_time = time.time()

    for epoch in range(CONFIG['epochs']):
        # 1. è®­ç»ƒ
        train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer, device, epoch)

        # 2. éªŒè¯
        val_loss, val_acc = evaluate(model, testloader, criterion, device)

        # 3. è®°å½•å†å²æ•°æ®
        record_layer_l2_norms(model, epoch, l2_history)

        # 4. ã€å®æ—¶è¾“å‡ºã€‘ è®¡ç®—å¹¶æ‰“å°æœ¬è½®çš„åˆ†å±‚ç»“æœ
        critical_layers, robust_layers, thresh = classify_layers_realtime(model)

        print(f"\n[{epoch + 1}/{CONFIG['epochs']}] Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%")
        print(f"=" * 20 + " åŠ¨æ€åˆ†å±‚ (Only L2) " + "=" * 20)
        print(f"åˆ’åˆ†é˜ˆå€¼ (Threshold): {thresh:.4f}")
        print(f"ğŸ”´ å…³é”®å±‚ (Critical/TCP, Count={len(critical_layers)}):")
        # æ‰“å°å‰5ä¸ªå’Œå5ä¸ªï¼Œé¿å…åˆ·å±ï¼Œæˆ–è€…æ ¹æ®å±‚æ•°å†³å®šæ˜¯å¦å…¨æ‰“å°
        # è¿™é‡Œä¸ºäº†ç›´è§‚ï¼Œå¦‚æœå±‚æ•°ä¸å¤šåˆ™å…¨æ‰“å°ï¼ŒVGGå±‚æ•°è¾ƒå¤šï¼Œæˆ‘ä»¬å¯ä»¥ç´§å‡‘æ‰“å°
        print(", ".join([x.split(' ')[0] for x in critical_layers]))  # åªæ‰“å°åå­—

        print(f"ğŸŸ¢ é²æ£’å±‚ (Robust/UDP, Count={len(robust_layers)}):")
        # æ‰“å°é²æ£’å±‚çš„æ•°é‡å’Œç®€è¦ä¿¡æ¯
        print(f"Includes {len(robust_layers)} layers with L2 < {thresh:.4f}")
        print("=" * 60 + "\n")

        scheduler.step()

    total_time = time.time() - start_time
    print(f"\nè®­ç»ƒç»“æŸï¼Œè€—æ—¶ {total_time / 60:.2f} åˆ†é’Ÿã€‚")
    save_and_plot_analysis(l2_history, CONFIG['analysis_path'])


if __name__ == '__main__':
    main()