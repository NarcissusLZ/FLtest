import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import os
import time
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import datetime
import copy

# å¯¼å…¥ä½ çš„æ¨¡å‹æ–‡ä»¶
# ç¡®ä¿ vgg16.py åœ¨åŒç›®å½•ä¸‹
from vgg16 import CIFAR10_VGG16, select_device

# ================= é…ç½®å‚æ•° =================
CONFIG = {
    'device': 'cuda',
    'epochs': 50,
    'batch_size': 64,
    'lr': 0.01,
    'momentum': 0.9,
    'weight_decay': 5e-4,
    'save_path': './checkpoints',
    'analysis_path': './analysis_results',
    'log_file': 'dual_factor_log.txt',  # æ—¥å¿—æ–‡ä»¶å
    'num_workers': 2,

    # === æ ¸å¿ƒç®—æ³•å‚æ•° (Innovation Points) ===
    'critical_ratio': 0.5,  # å…³é”®å±‚æ¯”ä¾‹ (Top 50%)

    # æ¢¯åº¦æƒé‡ (Gradient Importance Beta)
    # Score = Norm(Movement_Step) + beta * Norm(Gradient)
    # Movement_Step = W_t - W_{t-1} (å½“å‰è½®çš„å˜åŒ–å¹…åº¦)
    'grad_beta': 1.0
}


def get_data_loaders(batch_size, num_workers):
    """å‡†å¤‡ CIFAR-10 æ•°æ®é›†"""
    print("æ­£åœ¨å‡†å¤‡æ•°æ®...")
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    return trainloader, testloader


def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch + 1} Train', leave=False)

    for i, (inputs, labels) in pbar:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()  # åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        pbar.set_postfix({'Loss': f'{running_loss / (i + 1):.4f}', 'Acc': f'{100. * correct / total:.2f}%'})

    # æ³¨æ„ï¼šå‡½æ•°ç»“æŸæ—¶ï¼Œæ¨¡å‹å‚æ•° param.grad ä¸­ä¿ç•™äº†æœ€åä¸€ä¸ª Batch çš„æ¢¯åº¦
    # è¿™æ­£æ˜¯æˆ‘ä»¬ç”¨æ¥è®¡ç®—â€œæ•æ„Ÿåº¦â€çš„æœ€ä½³æ—¶æœº
    return running_loss / len(dataloader), 100. * correct / total


def evaluate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():  # éªŒè¯é˜¶æ®µä¸è®¡ç®—æ¢¯åº¦
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    return running_loss / len(dataloader), 100. * correct / total


# ================= è¾…åŠ©ç±»ï¼šåŒå› å­æŒ‡æ ‡è®¡ç®—å™¨ =================
class LayerMetricCalculator:
    """
    ä¿®æ”¹åé€»è¾‘ï¼š
    1. Movement: æƒé‡ç›¸å¯¹äºä¸Šä¸€è½®çš„å˜åŒ– (W_t - W_{t-1})
    2. Gradient: å½“å‰æ¢¯åº¦çš„èŒƒæ•° (Loss Sensitivity)
    """

    def __init__(self, model):
        # åˆå§‹åŒ–æ—¶ï¼Œprev_weights å°±æ˜¯åˆå§‹æƒé‡ W_0
        print("åˆå§‹åŒ– MetricCalculator: æ­£åœ¨å¤‡ä»½ä¸Šä¸€è½®æƒé‡(W_t-1)...")
        self.prev_weights = {}
        for name, param in model.named_parameters():
            if 'weight' in name and param.dim() > 1:
                # å­˜åˆ°CPUèŠ‚çœæ˜¾å­˜
                self.prev_weights[name] = param.data.clone().detach().cpu()

    def update_prev_weights(self, model):
        """
        æ¯è½®ç»“æŸåè°ƒç”¨ï¼Œå°†å½“å‰æƒé‡æ›´æ–°ä¸ºâ€œä¸Šä¸€è½®æƒé‡â€ï¼Œä¾›ä¸‹ä¸€è½®è®¡ç®—å·®å€¼ä½¿ç”¨
        """
        for name, param in model.named_parameters():
            if name in self.prev_weights:
                self.prev_weights[name] = param.data.clone().detach().cpu()

    def get_dual_metrics(self, model):
        """
        è·å–åŒå› å­åŸå§‹æ•°æ®
        """
        metrics_data = []

        for name, param in model.named_parameters():
            # åªå¤„ç†å·ç§¯å±‚å’Œå…¨è¿æ¥å±‚çš„æƒé‡
            if 'weight' not in name or param.dim() <= 1:
                continue

            # --- å› å­1: Movement (W_t - W_{t-1}) ---
            # ä¿®æ”¹ï¼šè®¡ç®—å½“å‰æƒé‡ä¸ prev_weights çš„å·®å€¼
            movement = 0.0
            if name in self.prev_weights:
                prev_w = self.prev_weights[name].to(param.device)
                movement = torch.norm(param.data - prev_w, p=2).item()

            # --- å› å­2: Gradient Norm (Sensitivity) ---
            grad_val = 0.0
            if param.grad is not None:
                grad_val = param.grad.norm(p=2).item()

            metrics_data.append({
                'name': name,
                'movement': movement,
                'grad': grad_val
            })

        return metrics_data


# ================= æ ¸å¿ƒé€»è¾‘ï¼šåŒå› å­èåˆåˆ†å±‚ç®—æ³• =================
def classify_layers_dual_factor(model, metric_calculator, ratio, grad_beta):
    """
    åŸºäº [Weight Increment] å’Œ [Gradient Sensitivity] çš„èåˆåˆ†å±‚ã€‚
    Score = Norm(W_t - W_{t-1}) + beta * Norm(Gradient)
    """
    # 1. è·å–åŸå§‹æ•°æ®
    raw_data = metric_calculator.get_dual_metrics(model)
    if not raw_data: return [], [], 0.0

    # 2. å‡†å¤‡å½’ä¸€åŒ–
    movements = [x['movement'] for x in raw_data]
    grads = [x['grad'] for x in raw_data]

    max_mov = max(movements) if movements and max(movements) > 0 else 1.0
    max_grad = max(grads) if grads and max(grads) > 0 else 1.0

    final_scores = []

    for item in raw_data:
        # Min-Max Normalization (Minå‡è®¾ä¸º0ï¼Œç®€åŒ–è®¡ç®—)
        norm_mov = item['movement'] / max_mov
        norm_grad = item['grad'] / max_grad

        # === æ ¸å¿ƒå…¬å¼ ===
        combined_score = norm_mov + (grad_beta * norm_grad)

        final_scores.append({
            'name': item['name'],
            'score': combined_score,
            'raw_mov': item['movement'],
            'raw_grad': item['grad']
        })

    # 3. æ’åº (Score è¶Šå¤§è¶Š Critical)
    final_scores.sort(key=lambda x: x['score'], reverse=True)

    # 4. åˆ‡åˆ† Top-K
    num_critical = int(len(final_scores) * ratio)
    if num_critical == 0 and ratio > 0: num_critical = 1  # è‡³å°‘ä¿ç•™ä¸€å±‚

    critical_list = final_scores[:num_critical]
    robust_list = final_scores[num_critical:]

    # 5. æ ¼å¼åŒ–è¾“å‡º (Name | Score | Movement | Gradient)
    critical_desc = [f"{x['name']} (S:{x['score']:.2f}|Mov:{x['raw_mov']:.4f}|G:{x['raw_grad']:.2f})" for x in
                     critical_list]
    robust_desc = [f"{x['name']} (S:{x['score']:.2f}|Mov:{x['raw_mov']:.4f}|G:{x['raw_grad']:.2f})" for x in
                   robust_list]

    threshold = critical_list[-1]['score'] if critical_list else 0.0

    return critical_desc, robust_desc, threshold


def log_and_print(message, log_path):
    print(message)
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(message + '\n')


def save_and_plot_analysis(history_list, save_dir):
    """ç®€å•çš„ç»˜å›¾å‡½æ•°ï¼Œè®°å½•ç»¼åˆå¾—åˆ†çš„å˜åŒ–"""
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    df = pd.DataFrame(history_list)
    csv_path = os.path.join(save_dir, 'layer_scores_history.csv')
    df.to_csv(csv_path, index=False)


# ================= ä¸»å‡½æ•° =================
def main():
    device = select_device(CONFIG['device'])
    print(f"Using device: {device}")

    if not os.path.exists(CONFIG['save_path']): os.makedirs(CONFIG['save_path'])
    if not os.path.exists(CONFIG['analysis_path']): os.makedirs(CONFIG['analysis_path'])
    log_path = os.path.join(CONFIG['analysis_path'], CONFIG['log_file'])

    # 1. æ¨¡å‹åˆå§‹åŒ–
    trainloader, testloader = get_data_loaders(CONFIG['batch_size'], CONFIG['num_workers'])
    model = CIFAR10_VGG16(num_classes=10).to(device)

    # 2. ã€å…³é”®æ­¥éª¤ã€‘ åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨ (ä¿å­˜ W_0 ä½œä¸ºç¬¬ä¸€è½®çš„ W_{t-1})
    metric_calc = LayerMetricCalculator(model)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=CONFIG['lr'], momentum=CONFIG['momentum'],
                          weight_decay=CONFIG['weight_decay'])
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)

    # è®°å½•æ—¥å¿—å¤´
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write(f"Training Log - {datetime.datetime.now()}\n")
        f.write(f"Strategy: Dual-Factor Metric (Incremental Movement + Beta*Gradient)\n")
        f.write(f"Params: Ratio={CONFIG['critical_ratio']}, Beta={CONFIG['grad_beta']}\n")
        f.write("=" * 60 + "\n")

    score_history = []
    start_time = time.time()

    log_and_print("å¼€å§‹è®­ç»ƒ...", log_path)

    for epoch in range(CONFIG['epochs']):
        # --- è®­ç»ƒ ---
        # è¿™é‡Œçš„ train_one_epoch ä¼šä¿ç•™æœ€åä¸€ä¸ª batch çš„æ¢¯åº¦
        train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer, device, epoch)

        # --- éªŒè¯ ---
        val_loss, val_acc = evaluate(model, testloader, criterion, device)

        # --- ã€æ ¸å¿ƒã€‘æ‰§è¡ŒåŒå› å­åˆ†å±‚ç®—æ³• ---
        # è¿™é‡Œçš„è®¡ç®—åŸºäºï¼šW_t (å½“å‰) - W_{t-1} (MetricCalcä¸­ä¿å­˜çš„)
        critical_layers, robust_layers, thresh = classify_layers_dual_factor(
            model,
            metric_calc,
            CONFIG['critical_ratio'],
            CONFIG['grad_beta']
        )

        # --- ã€é‡è¦ä¿®æ”¹ã€‘æ›´æ–° W_{t-1} ---
        # è®¡ç®—å®Œæœ¬è½®çš„åˆ†å±‚åï¼Œç«‹å³å°†å½“å‰æƒé‡ä¿å­˜ä¸º "ä¸Šä¸€è½®æƒé‡"ï¼Œä¾›ä¸‹ä¸€è½®ä½¿ç”¨
        metric_calc.update_prev_weights(model)

        # --- æ„å»ºæ—¥å¿— ---
        msg = []
        msg.append(f"\n[{epoch + 1}/{CONFIG['epochs']}] Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%")
        msg.append(f"=" * 10 + f" åŒå› å­åˆ†å±‚ (W_t-W_{{t-1}} + {CONFIG['grad_beta']}*Grad) " + "=" * 10)
        msg.append(f"å½“å‰åˆ†ç•Œ Score: {thresh:.4f}")

        msg.append(f"ğŸ”´ å…³é”®å±‚ (Critical/TCP, Count={len(critical_layers)}):")
        # æ‰“å°è¯¦ç»†ä¿¡æ¯: Name (Score|Mov|Grad)
        msg.append("\n".join(critical_layers))

        msg.append(f"\nğŸŸ¢ é²æ£’å±‚ (Robust/UDP, Count={len(robust_layers)}):")
        # é²æ£’å±‚åªæ‰“å°åå­—ç®€åŒ–æ˜¾ç¤º
        msg.append(", ".join([x.split(' ')[0] for x in robust_layers]))

        msg.append("=" * 60)
        log_and_print("\n".join(msg), log_path)

        scheduler.step()

    total_time = time.time() - start_time
    log_and_print(f"\nè®­ç»ƒç»“æŸï¼Œè€—æ—¶ {total_time / 60:.2f} åˆ†é’Ÿã€‚", log_path)

    # ä¿å­˜ç®€å•æ•°æ®
    save_and_plot_analysis(score_history, CONFIG['analysis_path'])


if __name__ == '__main__':
    main()