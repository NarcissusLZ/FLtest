import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import os
import time
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import datetime

from vgg16 import CIFAR100_VGG16, select_device

# ================= é…ç½®å‚æ•° =================
CONFIG = {
    'device': 'cuda',
    'epochs': 50,
    'batch_size': 64,
    'lr': 0.01,
    'momentum': 0.9,
    'weight_decay': 5e-4,
    'save_path': './checkpoints_cifar100',
    'analysis_path': './analysis_results_cifar100',
    'log_file': 'layer_split_log_refined.txt',  # æ–°æ—¥å¿—æ–‡ä»¶
    'num_workers': 2
}


def get_data_loaders(batch_size, num_workers):
    print("æ­£åœ¨å‡†å¤‡ CIFAR-100 æ•°æ®...")
    cifar100_mean = (0.5071, 0.4867, 0.4408)
    cifar100_std = (0.2675, 0.2565, 0.2761)
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(cifar100_mean, cifar100_std),
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(cifar100_mean, cifar100_std),
    ])
    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    return trainloader, testloader


def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch + 1} Train', leave=False)
    for i, (inputs, labels) in pbar:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        pbar.set_postfix({'Loss': f'{running_loss / (i + 1):.4f}', 'Acc': f'{100. * correct / total:.2f}%'})
    return running_loss / len(dataloader), 100. * correct / total


def evaluate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    return running_loss / len(dataloader), 100. * correct / total


def log_and_print(message, log_path):
    print(message)
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(message + '\n')


# ================= æ”¹è¿›çš„æ ¸å¿ƒç®—æ³•ï¼š3-Class Clustering & Filter BN =================

def kmeans_split_3_levels(values):
    """
    K-Means with k=3 (Low, Mid, High)
    è¿”å›ä¸¤ä¸ªé˜ˆå€¼: low_mid_thresh, mid_high_thresh
    """
    data = np.array(values).reshape(-1, 1)
    if len(data) < 3: return data[0][0]  # æ•°æ®å¤ªå°‘ä¸èšç±»

    # åˆå§‹åŒ–ä¸‰ä¸ªä¸­å¿ƒ: Min, Median, Max
    c1 = np.min(data)
    c2 = np.median(data)
    c3 = np.max(data)

    for _ in range(15):
        dist1 = np.abs(data - c1)
        dist2 = np.abs(data - c2)
        dist3 = np.abs(data - c3)

        # å½’ç±»
        labels = np.argmin(np.vstack((dist1.T, dist2.T, dist3.T)), axis=0)

        # æ›´æ–°ä¸­å¿ƒ
        new_c1 = data[labels == 0].mean() if np.any(labels == 0) else c1
        new_c2 = data[labels == 1].mean() if np.any(labels == 1) else c2
        new_c3 = data[labels == 2].mean() if np.any(labels == 2) else c3

        if c1 == new_c1 and c2 == new_c2 and c3 == new_c3:
            break
        c1, c2, c3 = new_c1, new_c2, new_c3

    # ç¡®ä¿ c1 < c2 < c3
    centers = sorted([c1, c2, c3])

    # æˆ‘ä»¬åªå…³å¿ƒ High ç»„çš„åˆ†ç•Œçº¿ (High Threshold)
    # å– Mid å’Œ High çš„ä¸­é—´ç‚¹ä½œä¸ºå…³é”®å±‚çš„é—¨æ§›
    critical_threshold = (centers[1] + centers[2]) / 2
    return critical_threshold


def classify_layers_refined(model):
    """
    æ”¹è¿›ç‰ˆåˆ†ç±»é€»è¾‘ï¼š
    1. æ’é™¤ 1D å‚æ•° (Batch Norm)ï¼Œåªåˆ†æ Conv å’Œ Dense
    2. ä½¿ç”¨ k=3 èšç±»ï¼Œåªæœ‰ Top Cluster è¢«åˆ¤å®šä¸º Critical
    """
    layer_scores = {}
    score_values = []

    for name, param in model.named_parameters():
        if 'weight' in name:
            # ã€å…³é”®ä¿®æ”¹ã€‘ å‰”é™¤ BN å±‚ (ndim=1)
            if len(param.shape) <= 1:
                continue

            l2_val = param.norm(p=2).item()
            num_params = param.numel()
            # è®¡ç®—èƒ½é‡å¯†åº¦ RMS
            rms_val = (l2_val / np.sqrt(num_params)) * 100

            layer_scores[name] = rms_val
            score_values.append(rms_val)

    # è®¡ç®—é«˜é˜¶é˜ˆå€¼ (ç­›é€‰çœŸæ­£çš„ Top Tier)
    threshold = kmeans_split_3_levels(score_values)

    critical = []
    robust = []

    for name, val in layer_scores.items():
        if val >= threshold:
            critical.append(f"{name} ({val:.2f})")
        else:
            robust.append(f"{name} ({val:.2f})")

    return critical, robust, threshold


def record_layer_metrics(model, epoch, history_list):
    for name, param in model.named_parameters():
        if 'weight' in name:
            # åŒæ ·åªè®°å½•ä¸»è¦å±‚
            if len(param.shape) <= 1: continue

            l2_val = param.norm(p=2).item()
            num_params = param.numel()
            rms_val = (l2_val / np.sqrt(num_params)) * 100

            history_list.append({
                'epoch': epoch + 1,
                'layer': name,
                'l2_norm': l2_val,
                'rms_score': rms_val
            })


def save_and_plot_analysis(history_list, save_dir):
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    df = pd.DataFrame(history_list)
    csv_path = os.path.join(save_dir, 'training_metrics_refined.csv')
    df.to_csv(csv_path, index=False)

    plt.figure(figsize=(12, 8))
    layers = df['layer'].unique()
    for layer_name in layers:
        layer_data = df[df['layer'] == layer_name]
        short_name = layer_name.replace('features.', 'F').replace('dense.', 'D').replace('classifier.', 'C')
        plt.plot(layer_data['epoch'], layer_data['rms_score'], label=short_name, marker='o', markersize=3)
    plt.title('Layer RMS Score Evolution (Conv & Dense Only)')
    plt.xlabel('Epoch')
    plt.ylabel('RMS Score')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'rms_refined_plot.png'), dpi=300)


# ================= ä¸»å‡½æ•° =================
def main():
    device = select_device(CONFIG['device'])
    print(f"Using device: {device}")

    if not os.path.exists(CONFIG['save_path']): os.makedirs(CONFIG['save_path'])
    if not os.path.exists(CONFIG['analysis_path']): os.makedirs(CONFIG['analysis_path'])

    log_path = os.path.join(CONFIG['analysis_path'], CONFIG['log_file'])
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write(f"Training Log (Refined) - {datetime.datetime.now()}\n")
        f.write("Strategy: RMS Score + BN Filtering + K-Means(k=3)\n")
        f.write("Target: Filter out small BN layers, select only Top-Tier Conv/Dense.\n")
        f.write("=" * 60 + "\n")

    trainloader, testloader = get_data_loaders(CONFIG['batch_size'], CONFIG['num_workers'])
    model = CIFAR100_VGG16(num_classes=100).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=CONFIG['lr'], momentum=CONFIG['momentum'],
                          weight_decay=CONFIG['weight_decay'])
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)

    history = []

    log_and_print(f"å¼€å§‹è®­ç»ƒ (Refined ç­–ç•¥)...", log_path)

    start_time = time.time()

    for epoch in range(CONFIG['epochs']):
        train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer, device, epoch)
        val_loss, val_acc = evaluate(model, testloader, criterion, device)

        record_layer_metrics(model, epoch, history)
        critical_layers, robust_layers, thresh = classify_layers_refined(model)

        msg = []
        msg.append(f"\n[{epoch + 1}/{CONFIG['epochs']}] Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%")
        msg.append(f"=" * 20 + " åŠ¨æ€åˆ†å±‚ (Refined) " + "=" * 20)
        msg.append(f"Top-Tier é˜ˆå€¼: {thresh:.4f}")
        msg.append(f"ğŸ”´ å…³é”®å±‚ (Critical/TCP, Count={len(critical_layers)}):")
        msg.append(", ".join([x.split(' ')[0] for x in critical_layers]))
        msg.append(f"ğŸŸ¢ é²æ£’å±‚ (Robust/UDP, Count={len(robust_layers)}):")
        # æ˜¾ç¤ºä¸€éƒ¨åˆ†é²æ£’å±‚
        if len(robust_layers) > 0:
            msg.append(", ".join([x.split(' ')[0] for x in robust_layers]))
        else:
            msg.append("None")
        msg.append("=" * 60)

        log_and_print("\n".join(msg), log_path)
        scheduler.step()

    total_time = time.time() - start_time
    log_and_print(f"\nè®­ç»ƒç»“æŸï¼Œè€—æ—¶ {total_time / 60:.2f} åˆ†é’Ÿã€‚", log_path)
    save_and_plot_analysis(history, CONFIG['analysis_path'])


if __name__ == '__main__':
    main()