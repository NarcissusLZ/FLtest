import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import os
import time
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import datetime
import copy

# å¯¼å…¥ä½ çš„æ¨¡å‹æ–‡ä»¶
# ç¡®ä¿ vgg16.py åœ¨åŒç›®å½•ä¸‹
from vgg16 import CIFAR10_VGG16, select_device

# ================= é…ç½®å‚æ•° =================
CONFIG = {
    'device': 'cuda',
    'epochs': 50,
    'batch_size': 64,
    'lr': 0.01,
    'momentum': 0.9,
    'weight_decay': 5e-4,
    'save_path': './checkpoints',
    'analysis_path': './analysis_results',
    'log_file': 'dual_factor_log.txt',  # æ—¥å¿—æ–‡ä»¶å
    'num_workers': 2,

    # === æ ¸å¿ƒç®—æ³•å‚æ•° (Innovation Points) ===
    'critical_ratio': 0.5,  # å…³é”®å±‚æ¯”ä¾‹ (Top 30%)

    # æ¢¯åº¦æƒé‡ (Gradient Importance Beta)
    # Score = Norm(Movement) + beta * Norm(Gradient)
    # beta=1.0 è¡¨ç¤ºâ€œå˜åŒ–é‡â€å’Œâ€œæ•æ„Ÿåº¦â€åŒç­‰é‡è¦
    # å¦‚æœä½ è®¤ä¸ºæ•æ„Ÿåº¦æ›´é‡è¦ï¼Œå¯ä»¥è®¾ä¸º 2.0
    'grad_beta': 1.0
}


def get_data_loaders(batch_size, num_workers):
    """å‡†å¤‡ CIFAR-10 æ•°æ®é›†"""
    print("æ­£åœ¨å‡†å¤‡æ•°æ®...")
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
    return trainloader, testloader


def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch {epoch + 1} Train', leave=False)

    for i, (inputs, labels) in pbar:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()  # åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        pbar.set_postfix({'Loss': f'{running_loss / (i + 1):.4f}', 'Acc': f'{100. * correct / total:.2f}%'})

    # æ³¨æ„ï¼šå‡½æ•°ç»“æŸæ—¶ï¼Œæ¨¡å‹å‚æ•° param.grad ä¸­ä¿ç•™äº†æœ€åä¸€ä¸ª Batch çš„æ¢¯åº¦
    # è¿™æ­£æ˜¯æˆ‘ä»¬ç”¨æ¥è®¡ç®—â€œæ•æ„Ÿåº¦â€çš„æœ€ä½³æ—¶æœº
    return running_loss / len(dataloader), 100. * correct / total


def evaluate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():  # éªŒè¯é˜¶æ®µä¸è®¡ç®—æ¢¯åº¦
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    return running_loss / len(dataloader), 100. * correct / total


# ================= è¾…åŠ©ç±»ï¼šåŒå› å­æŒ‡æ ‡è®¡ç®—å™¨ =================
class LayerMetricCalculator:
    """
    è´Ÿè´£åŒæ—¶è·Ÿè¸ªï¼š
    1. æƒé‡ç›¸å¯¹äºåˆå§‹åŒ–çš„ä½ç§» (Movement) -> ä»£è¡¨ 'Learned Feature Magnitude'
    2. å½“å‰æ¢¯åº¦çš„èŒƒæ•° (Gradient) -> ä»£è¡¨ 'Loss Sensitivity'
    """

    def __init__(self, model):
        # å†»ç»“å¹¶ä¿å­˜åˆå§‹æƒé‡å‰¯æœ¬ (W_0)
        print("åˆå§‹åŒ– MetricCalculator: æ­£åœ¨å¤‡ä»½åˆå§‹æƒé‡...")
        self.initial_weights = {}
        for name, param in model.named_parameters():
            if 'weight' in name and param.dim() > 1:
                # å­˜åˆ°CPUèŠ‚çœæ˜¾å­˜
                self.initial_weights[name] = param.data.clone().detach().cpu()

    def get_dual_metrics(self, model):
        """
        è·å–åŒå› å­åŸå§‹æ•°æ®
        """
        metrics_data = []

        for name, param in model.named_parameters():
            # åªå¤„ç†å·ç§¯å±‚å’Œå…¨è¿æ¥å±‚çš„æƒé‡
            if 'weight' not in name or param.dim() <= 1:
                continue

            # --- å› å­1: Movement (W_t - W_0) ---
            movement = 0.0
            if name in self.initial_weights:
                init_w = self.initial_weights[name].to(param.device)
                movement = torch.norm(param.data - init_w, p=2).item()

            # --- å› å­2: Gradient Norm (Sensitivity) ---
            grad_val = 0.0
            if param.grad is not None:
                grad_val = param.grad.norm(p=2).item()

            metrics_data.append({
                'name': name,
                'movement': movement,
                'grad': grad_val
            })

        return metrics_data


# ================= æ ¸å¿ƒé€»è¾‘ï¼šåŒå› å­èåˆåˆ†å±‚ç®—æ³• =================
def classify_layers_dual_factor(model, metric_calculator, ratio, grad_beta):
    """
    åŸºäº [Weight Movement] å’Œ [Gradient Sensitivity] çš„èåˆåˆ†å±‚ã€‚

    Logic:
    1. è·å–æ¯ä¸€å±‚çš„ Movement å’Œ Gradient å€¼ã€‚
    2. å¯¹ä¸¤è€…åˆ†åˆ«è¿›è¡Œ Min-Max å½’ä¸€åŒ– (æ˜ å°„åˆ° 0~1)ã€‚
    3. ç»¼åˆå¾—åˆ† Score = Norm(Movement) + beta * Norm(Gradient)ã€‚
    4. æ’åºå¹¶åˆ‡åˆ†ã€‚
    """
    # 1. è·å–åŸå§‹æ•°æ®
    raw_data = metric_calculator.get_dual_metrics(model)
    if not raw_data: return [], [], 0.0

    # 2. å‡†å¤‡å½’ä¸€åŒ–
    movements = [x['movement'] for x in raw_data]
    grads = [x['grad'] for x in raw_data]

    max_mov = max(movements) if movements and max(movements) > 0 else 1.0
    max_grad = max(grads) if grads and max(grads) > 0 else 1.0

    final_scores = []

    for item in raw_data:
        # Min-Max Normalization (Minå‡è®¾ä¸º0ï¼Œç®€åŒ–è®¡ç®—)
        norm_mov = item['movement'] / max_mov
        norm_grad = item['grad'] / max_grad

        # === æ ¸å¿ƒå…¬å¼ ===
        # æ—¢ä¿æŠ¤å˜åŒ–å¤§çš„ï¼Œä¹Ÿä¿æŠ¤æ¢¯åº¦å¤§çš„(æ•æ„Ÿçš„)
        combined_score = norm_mov + (grad_beta * norm_grad)

        final_scores.append({
            'name': item['name'],
            'score': combined_score,
            'raw_mov': item['movement'],
            'raw_grad': item['grad']
        })

    # 3. æ’åº (Score è¶Šå¤§è¶Š Critical)
    final_scores.sort(key=lambda x: x['score'], reverse=True)

    # 4. åˆ‡åˆ† Top-K
    num_critical = int(len(final_scores) * ratio)
    if num_critical == 0 and ratio > 0: num_critical = 1  # è‡³å°‘ä¿ç•™ä¸€å±‚

    critical_list = final_scores[:num_critical]
    robust_list = final_scores[num_critical:]

    # 5. æ ¼å¼åŒ–è¾“å‡º (Name | Score | Movement | Gradient)
    # ä¸ºäº†æ—¥å¿—æ•´æ´ï¼Œä¿ç•™ä¸¤ä½å°æ•°
    critical_desc = [f"{x['name']} (S:{x['score']:.2f}|M:{x['raw_mov']:.2f}|G:{x['raw_grad']:.2f})" for x in
                     critical_list]
    robust_desc = [f"{x['name']} (S:{x['score']:.2f}|M:{x['raw_mov']:.2f}|G:{x['raw_grad']:.2f})" for x in robust_list]

    threshold = critical_list[-1]['score'] if critical_list else 0.0

    return critical_desc, robust_desc, threshold


def log_and_print(message, log_path):
    print(message)
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(message + '\n')


def save_and_plot_analysis(history_list, save_dir):
    """ç®€å•çš„ç»˜å›¾å‡½æ•°ï¼Œè®°å½•ç»¼åˆå¾—åˆ†çš„å˜åŒ–"""
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    df = pd.DataFrame(history_list)
    csv_path = os.path.join(save_dir, 'layer_scores_history.csv')
    df.to_csv(csv_path, index=False)
    # è¿™é‡Œçœç•¥äº†å¤æ‚çš„ç»˜å›¾ä»£ç ï¼Œåªä¿å­˜æ•°æ®ï¼Œä»¥å…ä»£ç è¿‡é•¿


# ================= ä¸»å‡½æ•° =================
def main():
    device = select_device(CONFIG['device'])
    print(f"Using device: {device}")

    if not os.path.exists(CONFIG['save_path']): os.makedirs(CONFIG['save_path'])
    if not os.path.exists(CONFIG['analysis_path']): os.makedirs(CONFIG['analysis_path'])
    log_path = os.path.join(CONFIG['analysis_path'], CONFIG['log_file'])

    # 1. æ¨¡å‹åˆå§‹åŒ–
    trainloader, testloader = get_data_loaders(CONFIG['batch_size'], CONFIG['num_workers'])
    model = CIFAR10_VGG16(num_classes=10).to(device)

    # 2. ã€å…³é”®æ­¥éª¤ã€‘ åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨ (ä¿å­˜ W_0)
    metric_calc = LayerMetricCalculator(model)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=CONFIG['lr'], momentum=CONFIG['momentum'],
                          weight_decay=CONFIG['weight_decay'])
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)

    # è®°å½•æ—¥å¿—å¤´
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write(f"Training Log - {datetime.datetime.now()}\n")
        f.write(f"Strategy: Dual-Factor Metric (Movement + Beta*Gradient)\n")
        f.write(f"Params: Ratio={CONFIG['critical_ratio']}, Beta={CONFIG['grad_beta']}\n")
        f.write("=" * 60 + "\n")

    score_history = []
    start_time = time.time()

    log_and_print("å¼€å§‹è®­ç»ƒ...", log_path)

    for epoch in range(CONFIG['epochs']):
        # --- è®­ç»ƒ ---
        # è¿™é‡Œçš„ train_one_epoch ä¼šä¿ç•™æœ€åä¸€ä¸ª batch çš„æ¢¯åº¦
        train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer, device, epoch)

        # --- éªŒè¯ ---
        val_loss, val_acc = evaluate(model, testloader, criterion, device)

        # --- ã€æ ¸å¿ƒã€‘æ‰§è¡ŒåŒå› å­åˆ†å±‚ç®—æ³• ---
        # æ­¤æ—¶ model.parameters().grad ä¸­å­˜æœ‰æ¢¯åº¦çš„å€¼
        critical_layers, robust_layers, thresh = classify_layers_dual_factor(
            model,
            metric_calc,
            CONFIG['critical_ratio'],
            CONFIG['grad_beta']
        )

        # ç®€å•çš„è®°å½•å†å²ç”¨äºdebug (åªè®°ç¬¬ä¸€å±‚çš„åˆ†å€¼)
        # å®é™…ä½¿ç”¨å¯ä»¥æ‰©å±•è®°å½•æ‰€æœ‰å±‚
        # score_history.append(...)

        # --- æ„å»ºæ—¥å¿— ---
        msg = []
        msg.append(f"\n[{epoch + 1}/{CONFIG['epochs']}] Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%")
        msg.append(f"=" * 10 + f" åŒå› å­åˆ†å±‚ (Movement + {CONFIG['grad_beta']}*Grad) " + "=" * 10)
        msg.append(f"å½“å‰åˆ†ç•Œ Score: {thresh:.4f}")

        msg.append(f"ğŸ”´ å…³é”®å±‚ (Critical/TCP, Count={len(critical_layers)}):")
        # æ‰“å°è¯¦ç»†ä¿¡æ¯: Name (Score|Mov|Grad)
        # ä½¿ç”¨ join æ¢è¡Œæ‰“å°å‰å‡ ä¸ªï¼Œé¿å…å¤ªé•¿
        msg.append("\n".join(critical_layers))

        msg.append(f"\nğŸŸ¢ é²æ£’å±‚ (Robust/UDP, Count={len(robust_layers)}):")
        # é²æ£’å±‚åªæ‰“å°åå­—ç®€åŒ–æ˜¾ç¤º
        msg.append(", ".join([x.split(' ')[0] for x in robust_layers]))

        msg.append("=" * 60)
        log_and_print("\n".join(msg), log_path)

        scheduler.step()

    total_time = time.time() - start_time
    log_and_print(f"\nè®­ç»ƒç»“æŸï¼Œè€—æ—¶ {total_time / 60:.2f} åˆ†é’Ÿã€‚", log_path)

    # ä¿å­˜ç®€å•æ•°æ®
    save_and_plot_analysis(score_history, CONFIG['analysis_path'])


if __name__ == '__main__':
    main()